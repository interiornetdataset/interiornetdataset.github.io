<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-9795741-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-9795741-6');
</script>


	<title>InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset</title>
	<meta name="author" content="Wenbin Li">
	<link rel="stylesheet" type="text/css" href="./items/styles.css">
	<link rel="stylesheet" type="text/css" href="./items/thickbox.css" media="screen">
    <script type="text/javascript" src="./items/jquery.js" charset="utf-8"></script><style type="text/css"></style>
    <script type="text/javascript" src="./items/thickbox.js" charset="utf-8"></script>
</head>
<body screen_capture_injected="true">
	<div id="centerwrap">
	<div id="innerwrap">
		<div id="head">
			<h1>InteriorNet: Mega-scale Multi-sensor Photo-realistic <br> Indoor Scenes Dataset</h1>
			<h3><a href="https://wbli.me"><strong>Wenbin Li</strong></a>, 
				<a href="https://www.sajad-saeedi.ca"><strong>Sajad Saeedi</strong></a>, 
				<a href="https://wp.doc.ic.ac.uk/bjm113"><strong>John McCormac</strong></a>, 
				<a href="http://www.ronnieclark.co.uk"><strong>Ronald Clark</strong></a>, 
				<a href="https://www.dimostz.info"><strong>Dimos Tzoumanikas</strong></a>, <br> 
				<a href=""><strong>Qing Ye</strong></a>, 
		   		<a href="http://www.yuzhonghuang.org"><strong>Yuzhong Huang</strong></a>, 
				<a href="https://scholar.google.com/citations?user=dwvfKSkAAAAJ"><strong>Rui Tang</strong></a> and 
				<a href="http://wp.doc.ic.ac.uk/sleutene"><strong>Stefan Leutenegger</strong></a></h3>
            <h4>Imperial College London&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kujiale.com</h4>
            <br>
		</div>
		<div class="hr"><!-- --></div>
		<br>
		<br>

		<div id="content">
			    <iframe width="782" height="440" src="https://www.youtube.com/embed/z8uJh_xUq7A?rel=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
				<br><br><br><br>

<center style="">

				<table bgcolor="#808080" border="0" width="100%"><tbody><tr>
				<td><font color="#EBEBEB" size="3" face="verdana"><strong>Dataset Overview</strong></font></td>
				</tr></tbody></table> <br>

				<table>
				<tbody><tr><td><img src="./items/InteriorNet.jpg" width="750"></td></tr>
				</tbody></table><br>
				<div align="justify">System Overview: an end-to-end pipeline to render an RGB-D-inertial benchmark for large scale interior scene understanding and mapping. Our dataset contains <strong>20M</strong> images created by pipeline: (<strong>A</strong>) We collect around 1 million CAD models provided by world-leading furniture manufacturers. These models have been used in the real-world production. (<strong>B</strong>) Based on those models, around 1,100 professional designers create around 22 million interior layouts. Most of such layouts have been used in real-world decorations. (<strong>C</strong>) For each layout, we generate a number of configurations to represent different random lightings and simulation of scene change over time in daily life. (<strong>D</strong>) We provide an interactive simulator (<em>ViSim</em>) to help for creating ground truth IMU, events, as well as monocular or stereo camera trajectories including hand-drawn, random walking and neural network based realistic trajectory. (<strong>E</strong>) All supported image sequences and ground truth. </div><br><br>
				
				
				<table bgcolor="#808080" border="0" width="100%"><tbody><tr>
				<td><font color="#EBEBEB" size="3" face="verdana"><strong>Citation</strong></font></td>
				</tr></tbody></table>
					<pre><p align="left">
@inproceedings { InteriorNet18,
      author = { Wenbin Li and Sajad Saeedi and John McCormac and Ronald Clark and 
                 Dimos Tzoumanikas and Qing Ye and Yuzhong Huang and Rui Tang and 
                 Stefan Leutenegger },
   booktitle = { British Machine Vision Conference (BMVC) },
       title = { InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset },
        year = { 2018 }
}
					</p></pre>

				<table bgcolor="#808080" border="0" width="100%"><tbody><tr>
				<td><font color="#EBEBEB" size="3" face="verdana"><strong>Download</strong></font></td>
				</tr></tbody></table>

			<p align="justify"><br>
		  For access to our full image dataset, please agree to the terms of use [<a href="https://docs.google.com/forms/d/15sjV-CAud1ENBxK-hJSSrweE7IGc8VXWDbgZiq1trGc"><strong>HERE</strong></a>]. After we receive your agreement form, we will send you the links to our full image dataset. Many thanks for your patience. If you use any part of our dataset/code, please cite our paper. For more details, please refer to our [<a href="items/interiornet_paper.pdf"><strong>PAPER</strong></a>] [<a href="items/interiornet_format.pdf"><strong>FORMAT</strong></a>]. Any request or query should go to <a href="mailto:interiornetdataset@gmail.com"><strong>interiornetdataset[AT]gmail.com</strong></a> </p>
		<br>
		<p align="justify"><font style="color:red;"><strong>UPDATE:</strong></font> <a href="Kujiale.com"><strong>Kujiale.com</strong></a> will reserve the right of the assets including furniture models, layouts and scenes, used in this dataset. Please contact Dr. Rui Tang (<a href="mailto:ati@qunhemail.com"><strong>ati[AT]qunhemail.com</strong></a>) for the purpose of permission and download of the assets.
	</p></p><br>

		<table bgcolor="#808080" border="0" width="100%"><tbody><tr>
<td><font color="#EBEBEB" size="3" face="verdana"><strong>Acknowledgements</strong></font></td>
</tr></tbody></table> <br>
		<p align="justify">
			We would like to thank <a href="Kujiale.com"><strong>Kujiale.com</strong></a> for providing their database of production furniture models and layouts, as well as access to their GPU/CPU clusters. We also thank the Kujiale artists and other professionals for their great efforts into editing and labelling millions of models and scenes. We also highly appreciate comments and technical support from <em>Kujiale Rendering Group</em>, as well as helpful discussions and comments from Prof. <a href="https://www.doc.ic.ac.uk/~ajd"><strong>Andrew Davison</strong></a> and other members of <em>Robot Vision Group</em> of Imperial College London. This research is supported by the EPSRC grants PAMELA <a href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/K008730/1"><strong>EP/K008730/1</strong></a>, Aerial ABM <a href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/N018494/1"><strong>EP/N018494/1</strong></a>, and Imperial College London.
		</p>

<br>
		
		  </center>
			<br><br><br>
		</div>
	
		<div id="foot">
		  <p>Last updated: 4th September 2018</p></div>
	</div>
	
	</div>
	
	


</body></html>